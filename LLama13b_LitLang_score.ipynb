{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dcfcd2-9622-4939-b1bc-deedf7a2b36f",
   "metadata": {},
   "source": [
    "#### Import required packages and set the up the caches to be in the correct place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63239463-82e1-41b2-8159-cabb4e5b44cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/research/TopDownVideo/hw01558/conda/envs/dis/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/user/HS401/hw01558/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/vol/research/TopDownVideo/hw01558/conda/envs/dis/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HF_HOME\"] = \"/.cache/hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/.cache/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/.cache/datasets\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/.cache/hub\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3e844-8dbd-4388-b3e9-158bb237b08c",
   "metadata": {},
   "source": [
    "#### Login to hugging face\n",
    "#### Requires a token that allows access to the model, needs to be requested on hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1f39c8-83bb-4a2c-8ab0-56fd61de369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2a79e-889a-4993-a5c2-4635384fed4e",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd257a-9792-4da7-9865-9dc1597587eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\" \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230f178-3e28-4740-bd26-5b5f138c30b6",
   "metadata": {},
   "source": [
    "#### Load the stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3979f2cd-73bb-4ba1-ae67-77b9e814fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'NewYorkerStories.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    stories = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53362c68-3cdd-4842-ab78-c63b7e285a81",
   "metadata": {},
   "source": [
    "#### Load the instructions to make the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ee7ccb-83a3-441a-903b-ce3d1a30eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'LanguageAndLitDevices'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    instructions = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675871c-b5b9-4fdd-b77c-5aace8535bee",
   "metadata": {},
   "source": [
    "#### Function that combines instruction and story to create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f73e6d8-54ef-4b84-b676-29ccab70ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(story, instruction):\n",
    "    prompt = f\"\"\"    \n",
    "    You are a creative writing expert. {instruction}\n",
    "\n",
    "    Story:\n",
    "    {story}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af61c1-8b8d-4b5e-aa0f-955361c54a58",
   "metadata": {},
   "source": [
    "#### Main program: goes through each instruction and story, takes the response, splits the score and the reasoning, and then saves them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca471e-b2de-4327-bc61-17f98a3b6c5e",
   "metadata": {},
   "source": [
    "The stories are stored as a list of dictionaries. Each dictionary represents a single story and contains the following keys:\n",
    "\n",
    "content - the full text of the story\n",
    "\n",
    "story_id - name of author and number of story (e.g. 5_Claude)\n",
    "\n",
    "story_name - story title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8764f-d2e0-439e-a089-2aa247e2f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up output directory for results\n",
    "output_dir = \"instruction_tests/lang_and_lit/llama_13b/run_0\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Initialise variables\n",
    "scores = pd.DataFrame()\n",
    "i=0\n",
    "\n",
    "#Loop through each instruction\n",
    "for instruction in instructions:\n",
    "    i+=1\n",
    "    print('-------- Instruction ',i, '-------------')\n",
    "    #Loop through each story\n",
    "    for story in stories:\n",
    "        print('Story: ',story['story_id'])\n",
    "\n",
    "        #Reset cache and model to ensure space isn't overwhelmed\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "\n",
    "        #Build prompt\n",
    "        prompt = build_prompt(story['content'], instruction)\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(model.device) \n",
    "        input_token_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        #Generate response\n",
    "        outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.7,)\n",
    "    \n",
    "        generated_token_ids = outputs[0][input_token_length:]\n",
    "        response = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Find the score from in the response\n",
    "        match = re.search(r'Score:\\s*(\\d+)', response)\n",
    "        if match:\n",
    "            score = int(match.group(1))\n",
    "        else:\n",
    "            score = \"N/A\"\n",
    "\n",
    "        #Save score\n",
    "        new_row = pd.DataFrame([{\"story\": story['story_id'].split(\"_\",1)[0], \"writer\": story['story_id'].split(\"_\",1)[1], \"promt\": i, \"score\":score}])\n",
    "        scores = pd.concat([scores, new_row], ignore_index=True)\n",
    "           \n",
    "        # Save score and response\n",
    "        output_path = os.path.join(output_dir, f\"{story['story_id']}_instruction_{i}.json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump({\"story_name\": story['story_name'],'story_writer':story['story_id'], 'instruction':instruction, \"response\": response}, f, indent=2)\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "#Save all the scores to csv        \n",
    "scores.to_csv(os.path.join(output_dir,'scores.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda (dis)",
   "language": "python",
   "name": "dis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
